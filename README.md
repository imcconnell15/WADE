# WADE â€” Wide-Area Data Extraction

*A modular DFIR automation framework for staging, routing, and processing forensic artifacts at scaleâ€”built for austere ops, friendly to Splunk, and designed for real-world incident response.*

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## ğŸ¯ Overview

WADE is a comprehensive forensic artifact processing pipeline that automatically:
- **Classifies** incoming evidence (E01, memory dumps, VM images, disk images, network configs)
- **Routes** artifacts through appropriate forensic tools (Volatility, Dissect, Hayabusa, Plaso, YARA, Bulk Extractor)
- **Enriches** data with metadata, host context, and case information
- **Outputs** normalized JSONL for Splunk ingestion and analysis

### Key Features

âœ… **Idempotent & Auditable** â€” Deterministic installs, per-file JSON event logs, SQLite-backed deduplication  
âœ… **Modular Architecture** â€” Independent classifiers, workers, and routing engine  
âœ… **Configuration-Driven** â€” YAML + environment variables for flexible tool selection  
âœ… **Online or Offline** â€” Works with pinned packages; air-gapped operation ready  
âœ… **Ops-Friendly** â€” systemd units, logrotate policies, comprehensive logging  
âœ… **Splunk-Native** â€” Direct integration with Splunk forwarders and indexes

---

## ğŸ—ï¸ Architecture

```mermaid
graph TB
    A[Staging Directories] --> B[StagingDaemon]
    B --> C[Classifier Registry]
    C --> D[Tool Router]
    D --> E[Ticket Builder]
    E --> F[Queue]
    F --> G[Queue Runner]
    G --> H[Worker Dispatch]
    H --> I1[Volatility Worker]
    H --> I2[Dissect Worker]
    H --> I3[Hayabusa Worker]
    H --> I4[Plaso Worker]
    H --> I5[YARA Worker]
    H --> I6[Bulk Extractor Worker]
    I1 --> J[JSONL Output]
    I2 --> J
    I3 --> J
    I4 --> J
    I5 --> J
    I6 --> J
    J --> K[Splunk]
```

### Core Components

1.  [Staging Daemon](https://github.com/imcconnell15/WADE/pull/staging/)Â --- File classification, metadata extraction, ticket generation
2.  [Worker Framework](https://github.com/imcconnell15/WADE/pull/wade_workers/)Â --- Tool execution engine with unified ticket schema
3.  [Configuration System](https://github.com/imcconnell15/WADE/pull/etc/)Â --- Centralized YAML and environment-based config
4.  [Splunk Integration](https://github.com/imcconnell15/WADE/pull/splunkapp/)Â --- Index definitions, props/transforms, dashboards
5.  [YARA Rules](https://github.com/imcconnell15/WADE/pull/yara/)Â --- Malware detection and IOC scanning

* * * * *

ğŸš€ Quick Start
--------------

### Prerequisites

-   Ubuntu 20.04+ or RHEL 8+ (Linux-first; Windows worker support planned)
-   Python 3.8+
-   Forensic tools: Volatility3, Dissect, Plaso, Hayabusa, YARA, Bulk Extractor (installed viaÂ `install.sh`)
-   Splunk Universal Forwarder (optional, for data forwarding)

### Installation

```source-shell
# Clone the repository
git clone https://github.com/imcconnell15/WADE.git
cd WADE

# Run the idempotent installer (bootstrap + config + services)
sudo -E bash ./install.sh

# Verify services
sudo systemctl status wade-staging.service
sudo systemctl status wade-queue@autopsy.service
```

The installer will:

-   Create user/group (`autopsy`Â by default)
-   Install dependencies and forensic tools
-   Deploy configuration toÂ `/etc/wade/`
-   Set up systemd services
-   Configure logrotate policies
-   Create directory structure underÂ `/home/autopsy/`

* * * * *

ğŸ“‚ Repository Structure
-----------------------

```
WADE/
â”œâ”€â”€ staging/              # Staging daemon and classifiers
â”‚   â”œâ”€â”€ classifiers/      # E01, Memory, Disk, VM, Network, Misc
â”‚   â”œâ”€â”€ config.py         # Environment-driven configuration
â”‚   â”œâ”€â”€ db.py             # SQLite deduplication tracking
â”‚   â”œâ”€â”€ file_ops.py       # File I/O utilities (read head/tail, entropy, text detection)
â”‚   â”œâ”€â”€ path_resolver.py  # Destination path construction
â”‚   â”œâ”€â”€ stage_daemon.py   # Main orchestrator
â”‚   â”œâ”€â”€ ticket_builder.py # Ticket generation
â”‚   â””â”€â”€ tool_routing.py   # Tool selection engine
â”œâ”€â”€ wade_workers/         # Worker framework
â”‚   â”œâ”€â”€ bin/              # Queue runner and utilities
â”‚   â”œâ”€â”€ wade_workers/     # Worker implementations
â”‚   â”‚   â”œâ”€â”€ bulkextractor_worker.py
â”‚   â”‚   â”œâ”€â”€ dissect_worker.py
â”‚   â”‚   â”œâ”€â”€ hayabusa_worker.py
â”‚   â”‚   â”œâ”€â”€ plaso_worker.py
â”‚   â”‚   â”œâ”€â”€ volatility_worker.py
â”‚   â”‚   â”œâ”€â”€ yara_worker.py
â”‚   â”‚   â”œâ”€â”€ hashing.py           # Multi-strategy content hashing
â”‚   â”‚   â”œâ”€â”€ logging.py           # Unified event logging
â”‚   â”‚   â”œâ”€â”€ module_config.py     # YAML+ENV config loader
â”‚   â”‚   â”œâ”€â”€ subprocess_utils.py  # Tool discovery and execution
â”‚   â”‚   â””â”€â”€ ticket_schema.py     # Canonical ticket format
â”‚   â””â”€â”€ systemd/          # Service units
â”œâ”€â”€ etc/                  # Configuration templates
â”‚   â”œâ”€â”€ config.yaml       # Tool routing and module configuration
â”‚   â”œâ”€â”€ wade.conf         # Default environment variables
â”‚   â”œâ”€â”€ wade.env          # Runtime environment settings
â”‚   â””â”€â”€ logrotate.d/      # Log rotation policies
â”œâ”€â”€ splunkapp/            # Splunk integration
â”‚   â”œâ”€â”€ SA-wade-search/   # Search-head app
â”‚   â”œâ”€â”€ TA-wade-indexer/  # Indexer configs
â”‚   â””â”€â”€ TA-wade-uf/       # Universal Forwarder configs
â”œâ”€â”€ yara/                 # YARA rules repository
â”œâ”€â”€ WHIFF/                # Optional AI-assisted analysis module
â”œâ”€â”€ malware/              # Malware extraction utilities
â”œâ”€â”€ stigs/                # Security hardening configurations
â”œâ”€â”€ trouble/              # Troubleshooting scripts and guides
â”œâ”€â”€ windows/              # Windows-specific tools (future)
â”œâ”€â”€ install.sh            # Idempotent installer
â””â”€â”€ README.md             # This file

```

* * * * *

ğŸ« Data Flow: From File to Splunk
---------------------------------

### 1\.Â StagingÂ (`/home/autopsy/Staging/`)

```
Staging/
â”œâ”€â”€ full/    # Full pipeline (all tools)
â””â”€â”€ light/   # Triage pipeline (minimal tools)

```

Drop evidence files into either folder. The staging daemon:

-   Waits for file stability (no size changes for 10s)
-   Verifies no open writers viaÂ `lsof`
-   Computes content signature (SHA256 of head+tail)
-   Classifies via priority-ordered registry
-   Extracts metadata (hostname, OS, filesystem, profile)
-   Builds destination path
-   Moves file atomically
-   Generates ticket with tool routing
-   Enqueues for worker processing

### 2\.Â ClassificationÂ (Priority Order)

| Priority | Classifier | Detects |
| --- | --- | --- |
| 10 | E01Classifier | EnCase EWF images viaÂ `ewfinfo` |
| 15 | DiskClassifier | Raw disks (GPT/MBR, NTFS, FAT32, ext) |
| 20 | MemoryClassifier | Memory dumps (HIBR, LiME, raw) + Volatility profile |
| 25 | VMClassifier | VM images (VMDK, VHD, VHDX, QCOW2, VDI, OVA, OVF) |
| 40 | NetworkConfigClassifier | Device configs (Cisco, Juniper, PAN, Fortinet) |
| 45 | MalwareClassifier | Suspected malware samples |
| 50 | NetworkDocumentClassifier | Network diagrams and documentation |
| 100 | MiscClassifier | Fallback (text/binary with extension mapping) |

### 3\.Â Tool RoutingÂ (Configurable)

Example routing for E01 image (full profile):

```source-yaml
routing:
  defaults:
    e01:
      full: [dissect, hayabusa, plaso, bulk_extractor, yara]
      light: [dissect, hayabusa]
```

Override hierarchy:

1.  `DEFAULT_MATRIX`Â (code defaults)
2.  `config.yaml`Â routing.defaults
3.  OS-specific overrides (Windows/Linux)
4.  Location-specific overrides
5.  Environment variables (`WADE_ROUTE_E01_FULL=+yara_mem,-autopsy`)
6.  Global enable/disable (`WADE_DISABLE_TOOLS=autopsy`)

### 4\.Â Ticket Generation

TicketMetadata:

```source-json
{
  "ticket_id": "550e8400-e29b-41d4-a716-446655440000",
  "hostname": "DESKTOP-ABC123",
  "classification": "e01",
  "os_family": "Windows",
  "source_file": "/home/autopsy/Staging/full/evidence.E01",
  "dest_path": "/home/autopsy/DataSources/e01/DESKTOP-ABC123/evidence.E01",
  "file_size_bytes": 52428800,
  "file_hash_sha256": "abc123...",
  "case_id": "2025-001",
  "analyst": "autopsy",
  "created_utc": "2025-12-16T20:00:00Z",
  "priority": 5,
  "tags": ["ransomware", "critical"]
}
```

WorkerConfig:

```source-json
{
  "profile": "full",
  "location": "datacenterA",
  "requested_tools": ["dissect", "hayabusa", "plaso", "yara"]
}
```

### 5\.Â Worker Execution

Each worker:

1.  Loads ticket from queue
2.  Validates metadata and file existence
3.  Resolves modules from config (e.g., Volatility plugins, Dissect parsers)
4.  Executes tool via standardizedÂ `run_tool()`Â utility
5.  Parses output (JSON, JSONL, CSV â†’ normalized JSONL)
6.  Wraps each record inÂ artifact envelope:

    ```source-json
    {
      "tool": "volatility",
      "module": "windows.pslist",
      "host": "DESKTOP-ABC123",
      "case_id": "2025-001",
      "os_family": "Windows",
      "source_file": "/path/to/memory.dmp",
      "collected_utc": "2025-12-15T18:30:00Z",
      "staged_utc": "2025-12-16T20:00:00Z",
      "processed_utc": "2025-12-16T20:05:00Z",
      "analyst": "autopsy",
      ... original record fields ...
    }
    ```

7.  Writes toÂ `DataSources/<sourcetype>/<hostname>/<tool>/<module>/<hostname>_<timestamp>.jsonl`
8.  Logs completion event with record count and duration

### 6\.Â Output Structure

```
DataSources/
â”œâ”€â”€ e01/
â”‚   â””â”€â”€ DESKTOP-ABC123/
â”‚       â”œâ”€â”€ dissect/
â”‚       â”‚   â”œâ”€â”€ filesystem/
â”‚       â”‚   â”‚   â””â”€â”€ DESKTOP-ABC123_20251216T200530Z.jsonl
â”‚       â”‚   â””â”€â”€ registry/
â”‚       â”‚       â””â”€â”€ DESKTOP-ABC123_20251216T200545Z.jsonl
â”‚       â”œâ”€â”€ hayabusa/
â”‚       â”‚   â””â”€â”€ detections/
â”‚       â”‚       â””â”€â”€ DESKTOP-ABC123_20251216T200600Z.jsonl
â”‚       â””â”€â”€ plaso/
â”‚           â””â”€â”€ timeline/
â”‚               â””â”€â”€ DESKTOP-ABC123_20251216T201000Z.jsonl
â”œâ”€â”€ memory/
â”‚   â””â”€â”€ SERVER-XYZ/
â”‚       â”œâ”€â”€ volatility/
â”‚       â”‚   â”œâ”€â”€ memory/
â”‚       â”‚   â”‚   â”œâ”€â”€ SERVER-XYZ_20251216T203000Z_pslist.jsonl
â”‚       â”‚   â”‚   â””â”€â”€ SERVER-XYZ_20251216T203030Z_netscan.jsonl
â”‚       â””â”€â”€ yara_mem/
â”‚           â””â”€â”€ scan/
â”‚               â””â”€â”€ SERVER-XYZ_20251216T203100Z.jsonl
â””â”€â”€ _queue/           # Work orders
    â”œâ”€â”€ e01/
    â”‚   â””â”€â”€ full/
    â”‚       â””â”€â”€ 550e8400-e29b-41d4-a716-446655440000.json
    â””â”€â”€ memory/
        â””â”€â”€ full/
            â””â”€â”€ 660f9511-f3ac-52e5-b827-557766551111.json

```

### 7\.Â Splunk Ingestion

Universal Forwarder monitors:

-   `DataSources/**/*/jsonl`Â â†’ sourcetypeÂ `wade:<tool>:<module>`
-   `/var/wade/logs/`Â â†’ sourcetypeÂ `wade:events`

Index mappings:

-   `wade_dissect`Â --- Filesystem and registry artifacts
-   `wade_volatility`Â --- Memory analysis results
-   `wade_hayabusa`Â --- Windows event log detections
-   `wade_plaso`Â --- Timeline data
-   `wade_yara`Â --- YARA rule hits
-   `wade_bulk_extractor`Â --- Extracted features (emails, URLs, credit cards)
-   `wade_events`Â --- Operational events (staging, classification, worker completion)

* * * * *

âš™ï¸ Configuration
----------------

### Environment Variables (`/etc/wade/wade.env`)

Core Paths:

```source-shell
WADE_OWNER_USER=autopsy
WADE_DATADIR=/home/autopsy/DataSources
WADE_STAGINGDIR=/home/autopsy/Staging
WADE_QUEUE_DIR=/home/autopsy/DataSources/_queue
WADE_LOG_DIR=/var/wade/logs
```

Staging Behavior:

```source-shell
WADE_STAGE_STABLE_SECONDS=10
WADE_STAGE_POLL_INTERVAL=30
WADE_STAGE_REQUIRE_CLOSE_WRITE=true
WADE_STAGE_VERIFY_NO_WRITERS=true
WADE_STAGE_RECURSIVE=false
WADE_STAGE_ACCEPT_DOCS=false
WADE_STAGE_AUTO_DEFRAG_E01=false
```

Tool Paths:

```source-shell
WADE_VOLATILITY3_PATH=/opt/volatility3/vol.py
WADE_EWFINFO_PATH=/usr/bin/ewfinfo
WADE_LSOF_PATH=/usr/bin/lsof
```

Tool Routing Overrides:

```source-shell
WADE_ROUTE_E01_FULL=dissect,hayabusa,plaso,+yara,-autopsy
WADE_DISABLE_TOOLS=autopsy,bulk_extractor
WADE_ENABLE_TOOLS=yara_mem
```

### YAML Configuration (`/etc/wade/config.yaml`)

Tool Routing:

```source-yaml
routing:
  defaults:
    e01:
      full: [dissect, hayabusa, plaso, bulk_extractor]
      light: [dissect, hayabusa]
    memory:
      full: [volatility, yara_mem]
      light: [volatility]

  os_overrides:
    windows:
      add:
        memory.full: [hayabusa]
    linux:
      remove:
        memory.full: [hayabusa]

  location_overrides:
    datacenterA:
      add:
        e01.full: [yara_mem]
```

Volatility Modules:

```source-yaml
volatility:
  modules:
    - windows.info
    - windows.pslist
    - windows.pstree
    - windows.cmdline
    - windows.netscan
    - windows.handles
    - windows.malfind
```

Dissect Plugins:

```source-yaml
dissect:
  modules:
    - filesystem
    - registry
    - evtx
    - prefetch
    - timeline
```

* * * * *

ğŸ”§ Service Management
---------------------

### Staging Daemon

```source-shell
# Status
sudo systemctl status wade-staging.service

# Start/Stop/Restart
sudo systemctl start wade-staging.service
sudo systemctl stop wade-staging.service
sudo systemctl restart wade-staging.service

# Logs
journalctl -u wade-staging -f

# Enable/Disable
sudo systemctl enable wade-staging.service
sudo systemctl disable wade-staging.service
```

### Queue Runner

```source-shell
# Status
sudo systemctl status wade-queue@autopsy.service

# Timer status (if using timer-based execution)
sudo systemctl status wade-queue.timer

# Manual ticket dispatch
/opt/wade/staging/bin/wade_queue_runner.py /path/to/ticket.json

# Logs
journalctl -u wade-queue@autopsy -f
```

* * * * *

ğŸ“Š Monitoring & Troubleshooting
-------------------------------

### Event Logs

Staging EventsÂ (`/var/wade/logs/staging/stage_YYYY-MM-DD.jsonl`):

```source-json
{
  "timestamp_utc": "2025-12-16T20:00:00.123Z",
  "event_type": "staged",
  "source": "staging_daemon",
  "status": "success",
  "file_path": "/home/autopsy/Staging/full/evidence.E01",
  "classification": "e01",
  "confidence": 0.95,
  "hostname": "DESKTOP-ABC123",
  "os_family": "Windows",
  "dest_path": "/home/autopsy/DataSources/e01/DESKTOP-ABC123/evidence.E01",
  "file_size_bytes": 52428800,
  "content_sig": "abc123...",
  "duration_sec": 2.5
}
```

Worker EventsÂ (`/var/wade/logs/workers/<tool>_YYYY-MM-DD.jsonl`):

```source-json
{
  "timestamp_utc": "2025-12-16T20:05:00.456Z",
  "event_type": "worker_complete",
  "source": "volatility_worker",
  "status": "success",
  "tool": "volatility",
  "module": "windows.pslist",
  "host": "DESKTOP-ABC123",
  "record_count": 127,
  "duration_sec": 45.2,
  "output_path": "/home/autopsy/DataSources/memory/DESKTOP-ABC123/volatility/memory/DESKTOP-ABC123_20251216T200500Z_pslist.jsonl"
}
```

### Validation Script

```source-shell
# Validate all tickets in queue
python3 wade_workers/wade_workers/validate_tickets.py /home/autopsy/DataSources/_queue --verbose

# Output:
# âœ“ 15 valid tickets
# âš  2 warnings (low priority, missing optional fields)
# âœ— 1 errors (missing destination file)
```

### Common Issues

Issue:Â Staging daemon not picking up files

```source-shell
# Check service status
sudo systemctl status wade-staging.service

# Verify permissions
ls -la /home/autopsy/Staging/full/
sudo chown -R autopsy:autopsy /home/autopsy/Staging/

# Check logs
tail -f /var/wade/logs/staging/stage_$(date +%Y-%m-%d).jsonl
```

Issue:Â Worker failing to execute tool

```source-shell
# Verify tool is installed and in PATH
which volatility3
/opt/volatility3/vol.py --help

# Check tool discovery
python3 -c "
from wade_workers.wade_workers.subprocess_utils import get_default_registry
reg = get_default_registry()
print(reg.find_tool('volatility'))
"

# Run worker manually with verbose logging
python3 wade_workers/bin/wade_queue_runner.py /path/to/ticket.json --verbose
```

Issue:Â Duplicate processing

```source-shell
# Check deduplication database
sqlite3 /var/wade/staging/staging.db "SELECT * FROM processed ORDER BY staged_utc DESC LIMIT 10;"

# Clear specific entry (use with caution)
sqlite3 /var/wade/staging/staging.db "DELETE FROM processed WHERE sig='path_based_sig';"
```

* * * * *

ğŸ›£ï¸ Roadmap
-----------

### Near-Term (Q1 2026)

-   [ ] Â Windows worker host support (KAPE, Zimmerman tools, RECmd)
-   [ ] Â Enhanced YARA rule management (auto-update, rule validation)
-   [ ] Â Malware extraction and sandboxing integration
-   [ ] Â Advanced deduplication (fuzzy hashing with ssdeep)
-   [ ] Â GeoIP enrichment for network artifacts

### Mid-Term (Q2-Q3 2026)

-   [ ] Â Parallel worker execution for independent tools
-   [ ] Â Threat intelligence integration (VirusTotal, MISP, OTX)
-   [ ] Â MITRE ATT&CK technique mapping
-   [ ] Â Timeline correlation across tools
-   [ ] Â Web UI for queue management and status monitoring
-   [ ] Â Autopsy integration

### Long-Term (Q4 2026+)

-   [ ] Â Distributed processing (multi-node worker pools)
-   [ ] Â Cloud storage backends (S3, Azure Blob)
-   [ ] Â Offline installation kit with pinned dependencies
-   [ ] Â Advanced analytics (ML-based anomaly detection)
-   [ ] Â Custom worker plugin framework
-   [ ] Â RESTful API for programmatic access

* * * * *

ğŸ“š Documentation
----------------

-   [Staging Daemon](https://github.com/imcconnell15/WADE/pull/staging/README.md)Â --- Classification engine and ticket generation
-   [Worker Framework](https://github.com/imcconnell15/WADE/pull/wade_workers/README.md)Â --- Tool execution and output normalization
-   [Configuration Guide](https://github.com/imcconnell15/WADE/pull/etc/README.md)Â --- Environment and YAML configuration
-   [Splunk Integration](https://github.com/imcconnell15/WADE/pull/splunkapp/README.md)Â --- Index setup and search examples
-   [YARA Rules](https://github.com/imcconnell15/WADE/pull/yara/README.md)Â --- Rule management and custom rules
-   [Troubleshooting Guide](https://github.com/imcconnell15/WADE/pull/trouble/README.md)Â --- Common issues and solutions

* * * * *

ğŸ¤ Contributing
---------------

Contributions are welcome! Please:

1.  Fork the repository
2.  Create a feature branch (`git checkout -b feature/amazing-feature`)
3.  Commit changes (`git commit -m 'Add amazing feature'`)
4.  Push to branch (`git push origin feature/amazing-feature`)
5.  Open a Pull Request

### Development Setup

```source-shell
# Clone and install dev dependencies
git clone https://github.com/imcconnell15/WADE.git
cd WADE

# Install in editable mode with dev extras
pip install -e ./wade_workers[dev]

# Run tests
pytest wade_workers/tests/

# Lint
ruff check staging/ wade_workers/
```

* * * * *

ğŸ“œ License
----------

MIT License --- seeÂ [LICENSE](https://github.com/imcconnell15/WADE/pull/LICENSE)Â for details.

* * * * *

ğŸ™ Acknowledgments
------------------

-   Built by DFIR practitioners for real-world incident response
-   Designed for both online and austere/air-gapped operations
-   Special thanks to the forensic tool developers: Volatility Foundation, Fox-IT (Dissect), Yamato Security (Hayabusa), and the YARA community
-   Shout-out toÂ Mr. SpeaksÂ for inspiration and guidance

* * * * *

ğŸ“ Support & Contact
--------------------

-   Issues:Â [GitHub Issues](https://github.com/imcconnell15/WADE/issues)
-   Discussions:Â [GitHub Discussions](https://github.com/imcconnell15/WADE/discussions)
-   Security:Â Report vulnerabilities privately via GitHub Security Advisories
